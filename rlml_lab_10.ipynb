{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amithrajiv/Hackathon/blob/main/rlml_lab_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2303A51901\n",
        "\n",
        "M.amith rajiv\n",
        "\n",
        "Batch:- 09"
      ],
      "metadata": {
        "id": "XrEvZB-BcItO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring Multi-Agent Reinforcement Learning environments and MARL algorithms.\n"
      ],
      "metadata": {
        "id": "2tZi0hg7cSbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random, time, numpy as np, torch, torch.nn as nn, torch.optim as optim\n",
        "try:\n",
        "    from pettingzoo.mpe import simple_spread_v3\n",
        "except Exception:\n",
        "    from pettingzoo.mpe2 import simple_spread_v3\n",
        "\n",
        "Dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, ob, ac, h=256):\n",
        "        super().__init__()\n",
        "        self.m = nn.Sequential(nn.Linear(ob, h), nn.ReLU(), nn.Linear(h, h), nn.ReLU(), nn.Linear(h, ac))\n",
        "    def forward(self, x):\n",
        "        return self.m(x)\n",
        "\n",
        "class Buf:\n",
        "    def __init__(self, cap=200_000):\n",
        "        self.s = np.zeros((cap,), dtype=object)\n",
        "        self.a = np.zeros((cap,), dtype=np.int64)\n",
        "        self.r = np.zeros((cap,), dtype=np.float32)\n",
        "        self.ns = np.zeros((cap,), dtype=object)\n",
        "        self.d = np.zeros((cap,), dtype=np.bool_)\n",
        "        self.i = 0\n",
        "        self.n = 0\n",
        "        self.cap = cap\n",
        "    def push(self, s, a, r, ns, d):\n",
        "        self.s[self.i] = s; self.a[self.i] = a; self.r[self.i] = r; self.ns[self.i] = ns; self.d[self.i] = d\n",
        "        self.i = (self.i + 1) % self.cap\n",
        "        self.n = min(self.n + 1, self.cap)\n",
        "    def sample(self, bs):\n",
        "        idx = np.random.randint(0, self.n, size=bs)\n",
        "        s = np.stack(self.s[idx])\n",
        "        a = torch.as_tensor(self.a[idx], dtype=torch.long, device=Dev)\n",
        "        r = torch.as_tensor(self.r[idx], dtype=torch.float32, device=Dev)\n",
        "        ns = np.stack(self.ns[idx])\n",
        "        d = torch.as_tensor(self.d[idx], dtype=torch.float32, device=Dev)\n",
        "        return s, a, r, ns, d\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "class DQN:\n",
        "    def __init__(self, ob, ac, lr=2.5e-4, gamma=0.99, tau=0.005):\n",
        "        self.q = Net(ob, ac).to(Dev)\n",
        "        self.t = Net(ob, ac).to(Dev)\n",
        "        self.t.load_state_dict(self.q.state_dict())\n",
        "        self.o = optim.Adam(self.q.parameters(), lr=lr)\n",
        "        self.l = nn.SmoothL1Loss()\n",
        "        self.g = gamma\n",
        "        self.tau = tau\n",
        "        self.ac = ac\n",
        "        self.buf = Buf()\n",
        "    def act(self, ob, eps):\n",
        "        if np.random.rand() < eps:\n",
        "            return np.random.randint(self.ac)\n",
        "        with torch.no_grad():\n",
        "            q = self.q(torch.as_tensor(ob, dtype=torch.float32, device=Dev).unsqueeze(0))\n",
        "            return int(q.argmax(1).item())\n",
        "    def upd(self, bs=256):\n",
        "        if len(self.buf) < bs:\n",
        "            return 0.0\n",
        "        s, a, r, ns, d = self.buf.sample(bs)\n",
        "        s = torch.as_tensor(s, dtype=torch.float32, device=Dev)\n",
        "        ns = torch.as_tensor(ns, dtype=torch.float32, device=Dev)\n",
        "        qv = self.q(s).gather(1, a.view(-1,1)).squeeze(1)\n",
        "        with torch.no_grad():\n",
        "            nq = self.t(ns).max(1).values\n",
        "            tg = r + (1.0 - d) * self.g * nq\n",
        "        loss = self.l(qv, tg)\n",
        "        self.o.zero_grad(); loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(self.q.parameters(), 10.0)\n",
        "        self.o.step()\n",
        "        with torch.no_grad():\n",
        "            for p, tp in zip(self.q.parameters(), self.t.parameters()):\n",
        "                tp.data.lerp_(p.data, self.tau)\n",
        "        return float(loss.item())\n",
        "\n",
        "def mk_env(seed=0, n=3, cyc=200, lr=0.5):\n",
        "    e = simple_spread_v3.parallel_env(N=n, local_ratio=lr, max_cycles=cyc, continuous_actions=False)\n",
        "    e.reset(seed=seed)\n",
        "    return e\n",
        "\n",
        "def seed_all(s):\n",
        "    random.seed(s); np.random.seed(s); torch.manual_seed(s)\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n",
        "\n",
        "def flat_obs(obs_dict):\n",
        "    return {k: np.asarray(v, dtype=np.float32).reshape(-1) for k, v in obs_dict.items()}\n",
        "\n",
        "def pz_reset(env, seed=None):\n",
        "    out = env.reset(seed=seed)\n",
        "    if isinstance(out, tuple):\n",
        "        obs, _ = out\n",
        "    else:\n",
        "        obs = out\n",
        "    return obs\n",
        "\n",
        "def train(\n",
        "    total_steps=300_000, seed=7, n_agents=3, start_eps=1.0, end_eps=0.05, eps_decay_steps=200_000,\n",
        "    batch_size=256, tau=0.005, gamma=0.99, lr=2.5e-4, max_cycles=200, local_ratio=0.5,\n",
        "    eval_every=25_000, eval_episodes=5\n",
        "):\n",
        "    seed_all(seed)\n",
        "    env = mk_env(seed=seed, n=n_agents, cyc=max_cycles, lr=local_ratio)\n",
        "    ids = env.possible_agents\n",
        "    obs = pz_reset(env, seed=seed)\n",
        "    obs = flat_obs(obs)\n",
        "    od = {i: obs[i].shape[0] for i in ids}\n",
        "    ad = {i: env.action_space(i).n for i in ids}\n",
        "    ag = {i: DQN(od[i], ad[i], lr=lr, gamma=gamma, tau=tau) for i in ids}\n",
        "    step = 0\n",
        "    ep_r = {i: 0.0 for i in ids}\n",
        "    ep = 0\n",
        "    t0 = time.time()\n",
        "    def eps_at(s):\n",
        "        if s >= eps_decay_steps: return end_eps\n",
        "        return end_eps + (start_eps - end_eps) * (1 - s/eps_decay_steps)\n",
        "    while step < total_steps:\n",
        "        eps = eps_at(step)\n",
        "        acts = {i: ag[i].act(obs[i], eps) for i in ids}\n",
        "        nobs, rew, term, trunc, inf = env.step(acts)\n",
        "        nobs = flat_obs(nobs) if len(nobs) else {}\n",
        "        dn = {i: bool(term[i] or trunc[i]) for i in ids}\n",
        "        for i in ids:\n",
        "            ag[i].buf.push(obs[i], acts[i], rew[i], nobs.get(i, obs[i]), dn[i])\n",
        "            ep_r[i] += rew[i]\n",
        "        obs = nobs if len(nobs) else obs\n",
        "        step += 1\n",
        "        for i in ids: ag[i].upd(batch_size)\n",
        "        if all(dn.values()):\n",
        "            ep += 1\n",
        "            if ep % 10 == 0:\n",
        "                m = float(np.mean(list(ep_r.values())))\n",
        "                print(f\"steps={step} episodes={ep} eps={eps:.3f} mean_return_per_agent={m:.3f} buf={[len(ag[a].buf) for a in ids]}\")\n",
        "            obs = pz_reset(env)\n",
        "            obs = flat_obs(obs)\n",
        "            ep_r = {i: 0.0 for i in ids}\n",
        "        if step % eval_every == 0:\n",
        "            r = evaluate(ag, n_episodes=eval_episodes, seed=seed+123, n_agents=n_agents, max_cycles=max_cycles, local_ratio=local_ratio)\n",
        "            print(f\"[EVAL] steps={step} avg_team_return={r:.3f} elapsed={time.time()-t0:.1f}s\")\n",
        "    return ag\n",
        "\n",
        "def evaluate(agents, n_episodes=5, seed=123, n_agents=3, max_cycles=200, local_ratio=0.5):\n",
        "    env = mk_env(seed=seed, n=n_agents, cyc=max_cycles, lr=local_ratio)\n",
        "    ids = env.possible_agents\n",
        "    team = []\n",
        "    for e in range(n_episodes):\n",
        "        obs = pz_reset(env, seed=seed+e)\n",
        "        obs = flat_obs(obs)\n",
        "        ep_r = {i: 0.0 for i in ids}\n",
        "        dn = {i: False for i in ids}\n",
        "        while not all(dn.values()):\n",
        "            acts = {i: agents[i].act(obs[i], eps=0.0) for i in ids}\n",
        "            nobs, rew, term, trunc, inf = env.step(acts)\n",
        "            nobs = flat_obs(nobs) if len(nobs) else {}\n",
        "            dn = {i: bool(term[i] or trunc[i]) for i in ids}\n",
        "            for i in ids: ep_r[i] += rew[i]\n",
        "            obs = nobs if len(nobs) else obs\n",
        "        team.append(sum(ep_r.values()))\n",
        "    return float(np.mean(team))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    agents = train(\n",
        "        total_steps=15000,\n",
        "        seed=42,\n",
        "        n_agents=3,\n",
        "        start_eps=1.0,\n",
        "        end_eps=0.05,\n",
        "        eps_decay_steps=12000,\n",
        "        batch_size=256,\n",
        "        tau=0.01,\n",
        "        gamma=0.99,\n",
        "        lr=3e-4,\n",
        "        max_cycles=200,\n",
        "        local_ratio=0.5,\n",
        "        eval_every=2500,\n",
        "        eval_episodes=5\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OI5ILRd-aq2v",
        "outputId": "83f01bbb-d8e4-43cb-9781-bc046151eb13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "steps=2000 episodes=10 eps=0.842 mean_return_per_agent=-178.439 buf=[2000, 2000, 2000]\n",
            "[EVAL] steps=2500 avg_team_return=-507.890 elapsed=34.6s\n",
            "steps=4000 episodes=20 eps=0.683 mean_return_per_agent=-268.653 buf=[4000, 4000, 4000]\n",
            "[EVAL] steps=5000 avg_team_return=-504.387 elapsed=74.1s\n",
            "steps=6000 episodes=30 eps=0.525 mean_return_per_agent=-270.387 buf=[6000, 6000, 6000]\n",
            "[EVAL] steps=7500 avg_team_return=-679.346 elapsed=115.2s\n",
            "steps=8000 episodes=40 eps=0.367 mean_return_per_agent=-193.713 buf=[8000, 8000, 8000]\n",
            "steps=10000 episodes=50 eps=0.208 mean_return_per_agent=-210.869 buf=[10000, 10000, 10000]\n",
            "[EVAL] steps=10000 avg_team_return=-556.722 elapsed=154.7s\n",
            "steps=12000 episodes=60 eps=0.050 mean_return_per_agent=-208.017 buf=[12000, 12000, 12000]\n",
            "[EVAL] steps=12500 avg_team_return=-932.615 elapsed=195.4s\n",
            "steps=14000 episodes=70 eps=0.050 mean_return_per_agent=-172.557 buf=[14000, 14000, 14000]\n",
            "[EVAL] steps=15000 avg_team_return=-739.165 elapsed=236.0s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bkYuyHIsarVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}